{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Introduction\n\n<div style=\"color:white;display:fill;\n            background-color:#e38e05;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 4px;color:white;\"><b>1.1 Background</b></p>\n</div>\n\nWelcome to this competition on predicting **Cervical Spine Fractures** from **Computed Tomography (CT) scans**. The **motivation** to use AI for this task is that a **quick** diagnosis can **reduce** the chance of **neurologic deterioration** and paralysis after trauma.\n\n<center>\n<img src=\"https://www.holisticbodyworks.com.au/wp-content/uploads/2018/05/Thoracic-Spine.jpg\" width=400>\n</center>\n\n**Dataset origin**\n\nThe dataset we are using is made up of roughly **3000 CT studies**, [from twelve locations and across six continents](https://www.kaggle.com/competitions/rsna-2022-cervical-spine-fracture-detection/overview/acknowledgements). Spine **radiology specialists** have provided **annotations** to indicate the presence, vertebral level and location of any cervical spine fractures.\n\nSpecial thanks to the competition **hosts** for providing such a comprehensive dataset:\n* *Radiological Society of North America (RSNA)*\n* *American Society of Neuroradiology (ASNR)*\n* *American Society of Spine Radiology (ASSR)*\n\n<div style=\"color:white;display:fill;\n            background-color:#e38e05;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 4px;color:white;\"><b>1.2 Evaluation metric</b></p>\n</div>\n\nWe need to predict the **probability of fracture** for each of the **seven cervical vertebrae** denoted by C1, C2, C3, C4, C5, C6 and C7 as well as an **overall probability** of any fractures in the cervical spine. This means there will be **8 rows per image id**. Note that fractures in the skull base, thoracic spine, ribs, and clavicles are **ignored**.\n    \nThe **competition metric** is a **weighted multi-label logarithmic loss** (average across all rows)\n    \n$$\nL_{ij} = - w_j \\left(y_{ij} \\log(p_{ij}) + (1-y_{ij}) \\log(1-p_{ij})  \\right)\n$$\n\nwhere the **weights** [are given by](https://www.kaggle.com/competitions/rsna-2022-cervical-spine-fracture-detection/discussion/340392)\n    \n$$\nw_{j} = \\begin{cases}\n1, \\qquad \\text{if vertebrae negative} \\\\\n2, \\qquad \\text{if vertebrae positive} \\\\\n7, \\qquad \\text{if patient negative} \\\\\n14, \\qquad \\text{if patient negative}\n\\end{cases}\n$$\n\n<br>\n<center>\n<img src=\"https://i.postimg.cc/wBYCYqFG/metric-plot.png\" width=600>\n</center>\n<br>","metadata":{"execution":{"iopub.status.busy":"2022-08-17T05:12:35.870255Z","iopub.execute_input":"2022-08-17T05:12:35.871039Z","iopub.status.idle":"2022-08-17T05:12:35.878097Z","shell.execute_reply.started":"2022-08-17T05:12:35.870994Z","shell.execute_reply":"2022-08-17T05:12:35.876568Z"}}},{"cell_type":"markdown","source":"Notice how **more weight** is put on **positive cases** and and the **most weight** on the **overall probability** of any fractures.\n\n<hr>\n\n**Example:**\n\nBelow is a table going through an example of **how the evaluation metric is calculated** for a single case id. This would then be **averaged** across all row id's.\n\n<center>\n<img src=\"https://i.postimg.cc/nh2zCP5T/61415.jpg\" width=750>\n</center>\n\nNote: it is convention to use the **natural logarithm** (base e) for calculating the log loss. \n\n<div style=\"color:white;display:fill;\n            background-color:#e38e05;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 4px;color:white;\"><b>1.3 Code requirements</b></p>\n</div>\n\nThis is a **code competition**, which means that **submissions** are made through **notebooks**. Furthermore, the submission notebook is subject to these conditions:\n\n* **run-time** (CPU/GPU) **<= 9 hours**\n* **internet** access **disabled**\n* **external data is allowed**, including pre-trained models\n* the submission file must be named **submission.csv**\n              \nNote that the **test set is hidden**, and will populated when you submit your notebook. \n\n<hr>\n\n*Competition timeline:*\n    \n* Start date - 28th July 2022\n* Finish date - 27th October 2022\n\n<div style=\"color:white;display:fill;\n            background-color:#e38e05;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 4px;color:white;\"><b>1.4 Libraries</b></p>\n</div>","metadata":{}},{"cell_type":"code","source":"! pip install pylibjpeg pylibjpeg-libjpeg pydicom\n! pip install -U python-gdcm","metadata":{"execution":{"iopub.status.busy":"2022-08-23T05:18:24.457323Z","iopub.execute_input":"2022-08-23T05:18:24.457777Z","iopub.status.idle":"2022-08-23T05:18:45.603706Z","shell.execute_reply.started":"2022-08-23T05:18:24.457744Z","shell.execute_reply":"2022-08-23T05:18:45.602580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport seaborn as sns\nimport cv2\nimport os\nfrom os import listdir\nimport re\nimport gc\nimport gdcm\nimport pydicom\nfrom pydicom import dcmread\nimport pylibjpeg\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport scipy.ndimage\nfrom tqdm import tqdm\nfrom pprint import pprint\nfrom time import time\nimport itertools\nfrom skimage import measure \nfrom mpl_toolkits.mplot3d.art3d import Poly3DCollection\nimport nibabel as nib\nfrom glob import glob\nimport warnings\nimport dask.array as da\n%matplotlib inline\nsns.set(style='darkgrid', font_scale=1.6)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T05:18:45.606025Z","iopub.execute_input":"2022-08-23T05:18:45.606349Z","iopub.status.idle":"2022-08-23T05:18:45.622967Z","shell.execute_reply.started":"2022-08-23T05:18:45.606320Z","shell.execute_reply":"2022-08-23T05:18:45.622129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Data\n\n<div style=\"color:white;display:fill;\n            background-color:#e38e05;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 4px;color:white;\"><b>2.1 Data frames</b></p>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# Load metadata\ntrain_df = pd.read_csv(\"../input/rsna-2022-cervical-spine-fracture-detection/train.csv\")\ntrain_bbox = pd.read_csv(\"../input/rsna-2022-cervical-spine-fracture-detection/train_bounding_boxes.csv\")\ntest_df = pd.read_csv(\"../input/rsna-2022-cervical-spine-fracture-detection/test.csv\")\nss = pd.read_csv(\"../input/rsna-2022-cervical-spine-fracture-detection/sample_submission.csv\")\n\n# Print dataframe shapes\nprint('train shape:', train_df.shape)\nprint('train bbox shape:', train_bbox.shape)\nprint('test shape:', test_df.shape)\nprint('sample shape:', ss.shape)\nprint('')\n\n# Show first few entries\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T05:18:45.624352Z","iopub.execute_input":"2022-08-23T05:18:45.625009Z","iopub.status.idle":"2022-08-23T05:18:45.674176Z","shell.execute_reply.started":"2022-08-23T05:18:45.624971Z","shell.execute_reply":"2022-08-23T05:18:45.673169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **train.csv** - contains metadata for train_images.\n> * StudyInstanceUID - The study ID. There is one unique study ID for each patient scan.\n> * patient_overall - The patient level outcome, i.e. if any of the vertebrae are fractured.\n> * C[1-7] - Whether the given vertebrae is fractured.","metadata":{}},{"cell_type":"code","source":"train_bbox.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T05:18:45.676599Z","iopub.execute_input":"2022-08-23T05:18:45.677025Z","iopub.status.idle":"2022-08-23T05:18:45.690707Z","shell.execute_reply.started":"2022-08-23T05:18:45.676996Z","shell.execute_reply":"2022-08-23T05:18:45.689511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **train_bounding_boxes.csv** - contains bounding boxes of where fractures occured for a subset of the training set.\n> * StudyInstanceUID - The study ID. There is one unique study ID for each patient scan.\n> * x - x-coordinate of bounding box bottom left corner\n> * y - y-coordinate of bounding box bottom left corner\n> * width - width of bounding box\n> * height - height of bounding box\n> * slice_number - slice number of scan\n\nNote: we only have bounding boxes for a [subset](https://www.kaggle.com/competitions/rsna-2022-cervical-spine-fracture-detection/discussion/343105) of the training set. We'll explore the exact proportion later on.","metadata":{"execution":{"iopub.status.busy":"2022-08-17T05:35:18.507893Z","iopub.execute_input":"2022-08-17T05:35:18.509068Z","iopub.status.idle":"2022-08-17T05:35:18.518083Z","shell.execute_reply.started":"2022-08-17T05:35:18.509021Z","shell.execute_reply":"2022-08-17T05:35:18.516889Z"}}},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T05:18:45.692224Z","iopub.execute_input":"2022-08-23T05:18:45.692971Z","iopub.status.idle":"2022-08-23T05:18:45.706020Z","shell.execute_reply.started":"2022-08-23T05:18:45.692937Z","shell.execute_reply":"2022-08-23T05:18:45.705079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **test.csv** - contains metadata for test_images.\n> * row_id - The row ID. This will match the same column in the sample submission file.\n> * StudyInstanceUID - The study ID.\n> * prediction_type - Which one of the eight target columns needs a prediction in this row.\n\nNote: The full test set will be **populated at inference time**.","metadata":{}},{"cell_type":"code","source":"ss.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T05:18:45.707004Z","iopub.execute_input":"2022-08-23T05:18:45.707367Z","iopub.status.idle":"2022-08-23T05:18:45.721037Z","shell.execute_reply.started":"2022-08-23T05:18:45.707335Z","shell.execute_reply":"2022-08-23T05:18:45.719878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **sample_submission.csv** - A valid sample submission.\n> * row_id - The row ID. See the test.csv for what prediction needs to be filed in that row.\n> * fracture - The target column.","metadata":{}},{"cell_type":"markdown","source":"# 3. EDA - METADATA\n<div style=\"color:white;display:fill;\n            background-color:#e38e05;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 4px;color:white;\"><b>3.1 Fracture distributions</b></p>\n</div>\n\n* The overall target is roughly **balanced** (52/48 split). \n* **C7** has the **highest proportion of fractures** (19%) whereas **C3** has the **lowest** (4%) which is concistent with https://www.sciencedirect.com/topics/medicine-and-dentistry/third-cervical-vertebra. \n* Several patients have **more than one** fracture.\n* If **multiple fractures** occur on a single patient, they tend to occur in vertebrae **close together**, e.g. C4 & C5 as opposed to C1 & C7.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,6))\nplt.subplot(1,2,1)\nax1 = sns.countplot(data=train_df, x='patient_overall')\nfor container in ax1.containers:\n    ax1.bar_label(container)\nplt.title('Fractures by patient')\nplt.ylim([0,1300])\n\n# Unpivot train_df for plotting\ntrain_melt = pd.melt(train_df, id_vars = ['StudyInstanceUID', 'patient_overall'],\n             value_vars = ['C1','C2','C3','C4','C5','C6','C7'],\n             var_name=\"Vertebrae\",\n             value_name=\"Fractured\")\n\nplt.subplot(1,2,2)\nax2 = sns.countplot(data=train_melt, x='Vertebrae', hue='Fractured')\nfor container in ax2.containers:\n    ax2.bar_label(container)\nplt.title('Fractures by vertebrae')\nplt.ylim([0,2800])","metadata":{"execution":{"iopub.status.busy":"2022-08-23T05:18:45.722743Z","iopub.execute_input":"2022-08-23T05:18:45.723184Z","iopub.status.idle":"2022-08-23T05:18:46.205107Z","shell.execute_reply.started":"2022-08-23T05:18:45.723124Z","shell.execute_reply":"2022-08-23T05:18:46.204042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,5))\nax = sns.countplot(x = train_df[['C1','C2','C3','C4','C5','C6','C7']].sum(axis=1))\nfor container in ax.containers:\n    ax.bar_label(container)\nplt.title('Number of fractures by patient')\nplt.xlabel('Number of fractures')\nplt.ylim([0,1300])","metadata":{"execution":{"iopub.status.busy":"2022-08-23T05:18:46.206444Z","iopub.execute_input":"2022-08-23T05:18:46.206807Z","iopub.status.idle":"2022-08-23T05:18:46.478133Z","shell.execute_reply.started":"2022-08-23T05:18:46.206772Z","shell.execute_reply":"2022-08-23T05:18:46.477352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Heatmap of correlations\nplt.figure(figsize=(6,5))\nsns.heatmap(train_df[['C1','C2','C3','C4','C5','C6','C7']].corr(), cmap='bwr', vmin=-1, vmax=1)\nplt.title('Correlations')","metadata":{"execution":{"iopub.status.busy":"2022-08-23T05:18:46.479529Z","iopub.execute_input":"2022-08-23T05:18:46.480073Z","iopub.status.idle":"2022-08-23T05:18:46.762208Z","shell.execute_reply.started":"2022-08-23T05:18:46.480039Z","shell.execute_reply":"2022-08-23T05:18:46.761119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;\n            background-color:#e38e05;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 4px;color:white;\"><b>3.2 Study Id's</b></p>\n</div>\n\nThe cases in the dataset have **unique id's** like '1.2.826.0.1.3680043.6200'. It turns out only the **number after the last full stop** is important. ","metadata":{"execution":{"iopub.status.busy":"2022-08-17T06:03:12.961563Z","iopub.execute_input":"2022-08-17T06:03:12.962358Z","iopub.status.idle":"2022-08-17T06:03:12.969890Z","shell.execute_reply.started":"2022-08-17T06:03:12.962313Z","shell.execute_reply":"2022-08-17T06:03:12.968030Z"}}},{"cell_type":"code","source":"# Example\ntrain_df['StudyInstanceUID'][0]","metadata":{"execution":{"iopub.status.busy":"2022-08-23T05:18:46.766456Z","iopub.execute_input":"2022-08-23T05:18:46.766832Z","iopub.status.idle":"2022-08-23T05:18:46.773576Z","shell.execute_reply.started":"2022-08-23T05:18:46.766800Z","shell.execute_reply":"2022-08-23T05:18:46.772535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find unique numbers in study id's\nfor i in range(7):\n    print(train_df['StudyInstanceUID'].map(lambda x : x.split('.')[i]).unique())","metadata":{"execution":{"iopub.status.busy":"2022-08-23T05:18:46.775032Z","iopub.execute_input":"2022-08-23T05:18:46.775362Z","iopub.status.idle":"2022-08-23T05:18:46.797200Z","shell.execute_reply.started":"2022-08-23T05:18:46.775333Z","shell.execute_reply":"2022-08-23T05:18:46.795156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. EDA - Train images\n\n<div style=\"color:white;display:fill;\n            background-color:#e38e05;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 4px;color:white;\"><b>4.1 What is DICOM?</b></p>\n</div>\n\nA **.dcm** file follows the **Digital Imaging and Communications in Medicine** (DICOM) format. It is the standard format used for storing **medical images** and **related metadata**. It dates back to 1983, although it has been revised many times. \n\nThis **pixel size/coarseness** of the scan **differs** from scan to scan (e.g. the distance between slices may differ), which can **hurt performance** of CNN approaches. We can deal with this by **isomorphic resampling**.\n\nWe can use the [pydicom library](https://pydicom.github.io/) to open and explore these files.","metadata":{"execution":{"iopub.status.busy":"2022-08-17T06:12:50.206916Z","iopub.execute_input":"2022-08-17T06:12:50.207298Z","iopub.status.idle":"2022-08-17T06:12:50.214722Z","shell.execute_reply.started":"2022-08-17T06:12:50.207266Z","shell.execute_reply":"2022-08-17T06:12:50.212624Z"}}},{"cell_type":"code","source":"ex_path = \"../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.12281/110.dcm\"\ndcm_example = pydicom.dcmread(ex_path)\ndcm_example","metadata":{"execution":{"iopub.status.busy":"2022-08-23T05:18:46.799733Z","iopub.execute_input":"2022-08-23T05:18:46.800149Z","iopub.status.idle":"2022-08-23T05:18:46.813470Z","shell.execute_reply.started":"2022-08-23T05:18:46.800109Z","shell.execute_reply":"2022-08-23T05:18:46.812544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">The **image data** is stored in an array under **'Pixel Data'**. Everything else is **metadata**.\n>* The **'Rows'** and **'Columns'** values tell us the **image size**.\n>* The **'Pixel Spacing'** and **'Slice Thickness'** tell us the **pixel size** and **thickness**. Together the form a **voxel**.\n>* The **'Window Center'** and **'Window Width'** give information about the **brightness** and **contrast** of the image respectively.\n>* The **'Rescale Intercept'** and **'Rescale Slope'** determine the range of pixel values. ([ref](https://stackoverflow.com/questions/10193971/rescale-slope-and-rescale-intercept)) and to tensform the pixel value into **HU(Hounsfield Unit)**.\n>* **'ImagePositionPatient'** tells us the x, y, and z coordinates of the top left corner of each image in mm\n>* **InstanceNumber** is the slice number.","metadata":{}},{"cell_type":"markdown","source":"The unit of measurement in CT scans is the **Hounsfield Unit (HU)**, which is a measure of radiodensity. CT scanners are carefully calibrated to accurately measure this.  From Wikipedia:\n\n![HU examples][1]\n\nBy default however, the returned values are not in this unit. Let's fix this.\n\nSome scanners have cylindrical scanning bounds, but the output image is square. The pixels that fall outside of these bounds get the fixed value -2000. The first step is setting these values to 0, which currently corresponds to air. Next, let's go back to HU units, by multiplying with the rescale slope and adding the intercept (which are conveniently stored in the metadata of the scans!).\n\n  [1]: http://i.imgur.com/4rlyReh.png","metadata":{"execution":{"iopub.status.busy":"2022-08-17T06:34:50.471173Z","iopub.execute_input":"2022-08-17T06:34:50.471706Z","iopub.status.idle":"2022-08-17T06:34:50.484673Z","shell.execute_reply.started":"2022-08-17T06:34:50.471658Z","shell.execute_reply":"2022-08-17T06:34:50.483123Z"}}},{"cell_type":"code","source":"# Load the scans in given folder path\ndef load_scan(path):\n    slices = [pydicom.read_file(path + '/' + s) for s in os.listdir(path)]\n    slices.sort(key = lambda x: float(x.ImagePositionPatient[2])) # Sort the images based on the Z axis.\n    return slices","metadata":{"execution":{"iopub.status.busy":"2022-08-23T05:18:46.814585Z","iopub.execute_input":"2022-08-23T05:18:46.814864Z","iopub.status.idle":"2022-08-23T05:18:46.821233Z","shell.execute_reply.started":"2022-08-23T05:18:46.814838Z","shell.execute_reply":"2022-08-23T05:18:46.820405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path=\"../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.12281\" #train_df.loc[0,'StudyInstanceUID']\npatient=load_scan(path)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T05:18:46.822186Z","iopub.execute_input":"2022-08-23T05:18:46.822507Z","iopub.status.idle":"2022-08-23T05:18:47.895180Z","shell.execute_reply.started":"2022-08-23T05:18:46.822465Z","shell.execute_reply":"2022-08-23T05:18:47.894299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">### Adding further information\n\n>1. The CT-scan captures information about the radiodensity of an object or tissue exposed to x-rays. A transversal slice of a scan is reconstructed after taking measurements from several different directions.\n>2. We need to transform to Hounsfield units as the spectral composition of the x-rays depends on the measurement settings like acquisition parameters and tube voltage. By normalizing to values of water and air (water has HU 0 and air -1000) the images of different measurements are becoming comparable.\n>3. A ct-scanner yields roughly 4000 grey values that can't be captured by our eyes. This is why windowing is performed. This way the image is displayed in a HU range that suites most to the region of interest. ","metadata":{}},{"cell_type":"code","source":"def get_pixels_hu(slices):\n    image = np.stack([s.pixel_array for s in slices])\n    \n    # Convert to int16 (from sometimes int16), \n    # should be possible as values should always be low enough (<32k)\n    image = image.astype(np.int16)\n    image = da.from_array(image)\n    # Set outside-of-scan pixels to 0\n    # The intercept is usually -1024, so air is approximately 0\n    \n    \n    image[image <= -1000] = 0\n    \n    # Convert to Hounsfield units (HU)\n        \n    intercept = da.from_array([slices[slice_number].RescaleIntercept for slice_number in range(len(slices))])\n    slope = da.from_array([slices[slice_number].RescaleSlope for slice_number in range(len(slices))])\n    \n    intercept=intercept.reshape((-1,1,1))\n    slope=slope.reshape((-1,1,1))\n    \n    image= slope * image.astype(\"float64\")\n    image= image.astype(\"int16\")\n        \n    image+= intercept\n     \n    return image.astype(\"int16\")","metadata":{"execution":{"iopub.status.busy":"2022-08-23T05:18:47.896386Z","iopub.execute_input":"2022-08-23T05:18:47.896671Z","iopub.status.idle":"2022-08-23T05:18:47.904222Z","shell.execute_reply.started":"2022-08-23T05:18:47.896643Z","shell.execute_reply":"2022-08-23T05:18:47.903531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"patient_slice=get_pixels_hu(patient)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T05:18:47.905270Z","iopub.execute_input":"2022-08-23T05:18:47.905754Z","iopub.status.idle":"2022-08-23T05:18:48.486066Z","shell.execute_reply.started":"2022-08-23T05:18:47.905723Z","shell.execute_reply":"2022-08-23T05:18:48.485230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Ploting pixel array\nplt.imshow(patient[110].pixel_array,cmap='bone')\nplt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2022-08-23T05:18:48.487502Z","iopub.execute_input":"2022-08-23T05:18:48.488214Z","iopub.status.idle":"2022-08-23T05:18:48.692172Z","shell.execute_reply.started":"2022-08-23T05:18:48.488171Z","shell.execute_reply":"2022-08-23T05:18:48.690988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Ploting pixel array distribution\nplt.hist(patient[110].pixel_array.flatten(),color=\"r\",bins=50)\nplt.xlabel(\"Pixel Values\")\nplt.ylabel(\"Fequency\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T05:18:48.693756Z","iopub.execute_input":"2022-08-23T05:18:48.694219Z","iopub.status.idle":"2022-08-23T05:18:48.996257Z","shell.execute_reply.started":"2022-08-23T05:18:48.694176Z","shell.execute_reply":"2022-08-23T05:18:48.995546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Ploting HU array\nplt.imshow(patient_slice[110],cmap='bone')\nplt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2022-08-23T05:18:48.997176Z","iopub.execute_input":"2022-08-23T05:18:48.998200Z","iopub.status.idle":"2022-08-23T05:18:49.692577Z","shell.execute_reply.started":"2022-08-23T05:18:48.998157Z","shell.execute_reply":"2022-08-23T05:18:49.691456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Ploting HU distribution\nplt.hist(patient_slice[110].flatten().compute(),color=\"r\",bins=50)\nplt.xlabel(\"HU Values\")\nplt.ylabel(\"Fequency\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T05:18:49.694134Z","iopub.execute_input":"2022-08-23T05:18:49.694441Z","iopub.status.idle":"2022-08-23T05:18:50.448236Z","shell.execute_reply.started":"2022-08-23T05:18:49.694412Z","shell.execute_reply":"2022-08-23T05:18:50.446188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n<center>\n    <h1>Reference <b>Hounsfield Unit (HU)</b> values for vertebral column</h1>\n</center>\n<br>\n\n<br>\n<center>\n<img src=\"https://www.frontiersin.org/files/Articles/920167/fendo-13-920167-HTML/image_m/fendo-13-920167-t002.jpg\" width=800>\n</center>\n<br>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;\n            background-color:#e38e05;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 4px;color:white;\"><b>4.3 Normalization</b></p>\n</div>\n\nAs HU values range from **100** to **3000**. So we will use this values for normalization.","metadata":{}},{"cell_type":"code","source":"MIN_BOUND = 100\nMAX_BOUND = 3000.0\n    \ndef normalize(image):\n    image = (image - MIN_BOUND) / (MAX_BOUND - MIN_BOUND)\n    image[image>1] = 1.\n    image[image<0] = 0.\n    return image","metadata":{"execution":{"iopub.status.busy":"2022-08-23T05:18:50.449446Z","iopub.execute_input":"2022-08-23T05:18:50.449759Z","iopub.status.idle":"2022-08-23T05:18:50.456320Z","shell.execute_reply.started":"2022-08-23T05:18:50.449730Z","shell.execute_reply":"2022-08-23T05:18:50.455373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image=normalize(patient_slice)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T05:18:50.457471Z","iopub.execute_input":"2022-08-23T05:18:50.457743Z","iopub.status.idle":"2022-08-23T05:18:50.471789Z","shell.execute_reply.started":"2022-08-23T05:18:50.457717Z","shell.execute_reply":"2022-08-23T05:18:50.470708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(image[110],cmap=\"bone\")\nplt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2022-08-23T05:18:50.473237Z","iopub.execute_input":"2022-08-23T05:18:50.473629Z","iopub.status.idle":"2022-08-23T05:18:51.532617Z","shell.execute_reply.started":"2022-08-23T05:18:50.473598Z","shell.execute_reply":"2022-08-23T05:18:51.531546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;\n            background-color:#e38e05;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 4px;color:white;\"><b>4.4 Zero centering</b></p>\n</div>\n\nAs a final preprocessing step, it is advisory to **zero center** your data so that your **mean value is 0**. To do this you simply **subtract** the mean pixel value from all pixels.","metadata":{}},{"cell_type":"code","source":"PIXEL_MEAN = 0.016076984448095525\n\ndef zero_center(image):\n    image = image - PIXEL_MEAN\n    return image","metadata":{"execution":{"iopub.status.busy":"2022-08-23T05:18:51.533929Z","iopub.execute_input":"2022-08-23T05:18:51.534260Z","iopub.status.idle":"2022-08-23T05:18:51.540646Z","shell.execute_reply.started":"2022-08-23T05:18:51.534211Z","shell.execute_reply":"2022-08-23T05:18:51.539479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image=zero_center(image)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T05:18:51.541799Z","iopub.execute_input":"2022-08-23T05:18:51.542093Z","iopub.status.idle":"2022-08-23T05:18:51.551699Z","shell.execute_reply.started":"2022-08-23T05:18:51.542066Z","shell.execute_reply":"2022-08-23T05:18:51.550768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(image[110],cmap=\"bone\")\nplt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2022-08-23T05:18:51.553069Z","iopub.execute_input":"2022-08-23T05:18:51.553393Z","iopub.status.idle":"2022-08-23T05:18:52.616355Z","shell.execute_reply.started":"2022-08-23T05:18:51.553364Z","shell.execute_reply":"2022-08-23T05:18:52.615573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Delete all unused objects to free up memory\ndel ax1\ndel ax2\ndel ax\ndel ex_path\ndel dcm_example\ndel path\ndel patient\ndel patient_slice\ndel image\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T05:18:52.617757Z","iopub.execute_input":"2022-08-23T05:18:52.618395Z","iopub.status.idle":"2022-08-23T05:18:52.795822Z","shell.execute_reply.started":"2022-08-23T05:18:52.618353Z","shell.execute_reply":"2022-08-23T05:18:52.794668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#To preprocess all the patient folder and download it\nimport zipfile\nimport bz2\nimport _pickle as cPickle\n\ndef save_image():\n    pixel_sum=0\n    pixel_number=0\n    with zipfile.ZipFile('final.zip', 'w') as zipF:\n        for patient_ID in tqdm(train_df['StudyInstanceUID']):\n            path=\"../input/rsna-2022-cervical-spine-fracture-detection/train_images/\"+patient_ID\n            patient_pixel=load_scan(path)\n            patient_hu=get_pixels_hu(patient_pixel)\n            patient_hu_normalised=normalize(patient_hu)\n            patient_hu_normalised_centered=zero_center(patient_hu_normalised)\n            \n            with bz2.BZ2File(patient_ID+'.pbz2', 'w') as f: \n                cPickle.dump(patient_hu_normalised_centered, f)\n            zipF.write(patient_ID+'.pbz2', compress_type=zipfile.ZIP_DEFLATED, compresslevel=9)\n            os.remove(patient_ID+'.pbz2')\n            \n            del path\n            del patient_pixel\n            del patient_hu\n            del patient_hu_normalised\n            del patient_hu_normalised_centered\n            gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T05:18:52.801025Z","iopub.execute_input":"2022-08-23T05:18:52.802099Z","iopub.status.idle":"2022-08-23T05:18:52.809133Z","shell.execute_reply.started":"2022-08-23T05:18:52.802061Z","shell.execute_reply":"2022-08-23T05:18:52.808011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_image()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T05:18:52.810846Z","iopub.execute_input":"2022-08-23T05:18:52.811544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is great, but at the moment we don't know which vertebrae is being shown in each image. One way to work this out is by using the **segmentations**.","metadata":{}},{"cell_type":"markdown","source":"# 5. Segmentations\n\n<div style=\"color:white;display:fill;\n            background-color:#e38e05;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 4px;color:white;\"><b>5.1 What is NIfTI? </b></p>\n</div>\n\nA **.nii** file follows the **Neuroimaging Informatics Technology Initiative** (NIfTI) format. Compared to the DICOM, NIfTI is **simpler** and **easier** to support. \n\nTo open .nii files we can use the [nibabel library](https://nipy.org/nibabel/gettingstarted.html).","metadata":{"execution":{"iopub.status.busy":"2022-08-19T07:22:57.885627Z","iopub.execute_input":"2022-08-19T07:22:57.886895Z","iopub.status.idle":"2022-08-19T07:22:57.894865Z","shell.execute_reply.started":"2022-08-19T07:22:57.886847Z","shell.execute_reply":"2022-08-19T07:22:57.893215Z"}}},{"cell_type":"code","source":"def load_NIfTI(path):\n    nii_example = nib.load(path)\n\n    # Convert to numpy array\n    seg = nii_example.get_fdata()\n    print(seg.shape)\n    \n    return seg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path=\"../input/rsna-2022-cervical-spine-fracture-detection/segmentations/1.2.826.0.1.3680043.12281.nii\" #train_df.loc[0,'StudyInstanceUID']\nsegment_image=load_NIfTI(path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Each nifti file contains segmentations for **all slices** in a scan. However, we need to be careful about the **orientation** of the segmentations.\n\n> Please be aware that the NIFTI files consist of segmentation in the sagittal plane, while the DICOM files are in the axial plane.\n\nThe correct way to deal with this is explained in this [discussion post](https://www.kaggle.com/competitions/rsna-2022-cervical-spine-fracture-detection/discussion/340612). We need to use the following line:","metadata":{}},{"cell_type":"code","source":"# Align orientation with images\nsegment_image = segment_image[:, ::-1, ::-1].transpose(2, 1, 0)\nsegment_image.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<div style=\"color:white;display:fill;\n            background-color:#e38e05;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 4px;color:white;\"><b>5.2 Exploring the masks</b></p>\n</div>","metadata":{}},{"cell_type":"code","source":"# Plot images\nfig, axes = plt.subplots(nrows=3, ncols=6, figsize=(24,12))\nfig.suptitle(f'ID: 1.2.826.0.1.3680043.12281', weight=\"bold\", size=20)\n\nfor i in range(110,128):\n    mask = segment_image[i]\n    slice_no = i\n\n    # Plot the image\n    x = (i-110) // 6\n    y = (i-110) % 6\n\n    axes[x, y].imshow(mask, cmap='inferno')\n    axes[x, y].set_title(f\"Slice: {slice_no}\", fontsize=14, weight='bold')\n    axes[x, y].axis('off')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Compare these with the previous images. These masks give us the **location** of the vertebrae, which is very helpful because we know the fractures can only occur in these regions.\n\nThey also tells us which **vertebrae** are in the images. By looking at the unique values in each slice, we find a 0 for the background and another number like 3 for vertebrae C3. ","metadata":{}},{"cell_type":"code","source":"np.unique(segment_image[110])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Unfortunately, we don't have segmentations for the whole train set. This means we will have to extract the **vertebrae numbers** from the **.dcm files**. An idea to keep in mind is that maybe we could train a segmentation model to predict the segmentation masks for the rest of the train and all of the test images.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}